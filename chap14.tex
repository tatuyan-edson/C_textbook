ソートは、ある程度慣れたプログラマであれば常識として知っておいても良いアルゴリズムで、活用される場面も少なくない。また、アルゴリズムの解析においても、典型的かつ動作が見やすくイメージしやすいため、非常によく用いられる例である。本講では様々なソートを知り、アルゴリズムの解析手法についても学ぶと共に、これと関連の深い配列の探索についても学ぶ。ソート、サーチの世界はそれだけで分厚い本が書けるほどに奥深く\footnote{具体的には「The Art of Computer Programming Volume 3」(D.E.Knuth著、有澤 誠他訳、ASCII、2006)。ソート・サーチについてはこの本1冊で万全であると言って過言でない名著。}とても1講で解説しきれるものではないが、ここで素地だけでも学んでおくことが後々役立つと考えている。
\section{ソート総論}
個々のソートを見ていく前に、ソート全体について説明する。
\subsection{ソートとは何か}
\textbf{ソート}\index{そーと@ソート}(Sort)は、日本語では「整列」や「並び替え」と訳され、データをある一定の規則に従って並び替えることを言う。ソートの方法は多数存在し、それぞれに特徴がある。

この章のソートの部分では、代表的なソートのアルゴリズムのみを紹介し、紙数の関係上実装は行わない。そのため、これまでの復習として自力で実装してみていただきたい。なお、特別に断らない限り、ソート対象を要素数$n$のint型配列、ソート規則を昇順(小さい数が前)として説明を行う。
\subsection{ソートの解析〜バブルソートを例に〜}
アルゴリズムの解析について、本書ではここまで全く書いてこなかったので、ここでバブルソートを例に解析法を紹介しておく。なお、演習編では最初にアルゴリズムの解析について掲載したので、合わせて参照されたい。
\\ \\　
\textbf{バブルソート}\index{ばぶるそーと@バブルソート}(Bubble Sort)は、配列を前から順に見ていって、その大小関係がひっくり返っている場所をスワップすることで整列を行う方法である。例えば[4,1,3,2,3]のバブルソートを考えると、[1,4,3,2,3]$\rightarrow$[1,3,4,2,3]$\rightarrow$[1,3,2,4,3]$\rightarrow$[1,3,2,3,4]$\rightarrow$[1,2,3,3,4]というソート手順になる。小さい要素が泡のように上(前)にやってくることから、この名前がついた。

\minisec{時間計算量と空間計算量}
さて、バブルソートの特徴として、配列を一度スキャンし終えると、最大の要素が配列の末尾に来ることが挙げられる。そのため、今度は配列より1小さい数の比較ですむ。これを繰り返していくと、比較回数は$\frac{n(n+1)}{2}$回であり、交換回数は最大$\frac{n(n+1)}{2}$回であることがわかるだろう。このことから、バブルソートのアルゴリズムの計算回数は、$n$の二次関数に比例して大きくなっていくことがわかる。つまり、バブルソートにかかる時間は、十分$n$が大きい時、「高々」$n^2$に比例する程度である。これを$O(n^2)$のアルゴリズムと記す。
\\ \\　
ここで用いた$O$による記法を\textbf{ランダウの漸近記法}\index{らんだうのぜんきんきほう@ランダウの漸近記法}(Landau asymptotic notation)とよぶ。なお、ここでは「高々」の評価としたが、同じく$n$が十分大きい時ちょうど$f(n)$に比例するようなら$\Theta\left(f(n)\right)$と、少なくとも$g(n)$に比例するようなら$\Omega\left(g(n)\right)$と記す。以下では原則$O$を用いる。

また、これらの$O,\Theta,\Omega$は何れも使われるメモリの量を記すときにも使える。バブルソートの場合、元の配列以外で余分に使うメモリはせいぜい交換用の変数ぐらいで、$n$によらないので$O(1)$のメモリ量である。
\\ \\　
ここに述べた、計算回数のオーダーは一般に\textbf{時間計算量}\index{じかんけいさんりょう@時間計算量}(time complexity)と、メモリ量のオーダーは\textbf{空間計算量}\index{くうかんけいさんりょう@空間計算量}(space complexity)と呼ばれている。また、$O(1)$は\textbf{定数オーダー}\index{ていすうおーだー@定数オーダー}と、$O(\ln n)$は\textbf{対数オーダー}\index{たいすうおーだー@対数オーダー}と、$O(n^k)$は\textbf{多項式オーダー}\index{たこうしきおーだー@多項式オーダー}\footnote{ある問題に対し、その解を多項式時間で求められる問題を\textbf{Class-P}\index{Class-P}(P:Polynomial time)の問題と呼ぶ。また、ある問題の解の候補が与えられた時、それが解として正答かどうかを判定するのに多項式時間で判定できるものを\textbf{Class-NP}\index{Class-NP}(NP:Non-deterministic Polynomial time)の問題と呼ぶ。わかりやすく言うと、多項式時間で解ける問題がClass-P、多項式時間で丸付けのできる問題がClass-NPである。アメリカ・クレイ研究所のミレニアム懸賞問題の中には、これにまつわる問題として\textbf{P対NP問題}がある。この問題は、Class-NPに属する問題は必ずClass-Pに属するか？という問題である(逆にClass-Pの問題がClass-NPに属することは直感からも明らかだろう)。興味があれば調べてみて欲しい。}と、$O(e^n)$は\textbf{指数オーダー}\index{しすうおーだー@指数オーダー}と呼ばれる。時間計算量にこれらのオーダーが用いられた場合「○○オーダーの時間」と呼ぶのは煩わしいので「指数時間」などと呼ばれる。

オーダーについて注意しなければならない点として、十分大きい値での比例関係での等号なので定数倍やよりオーダーの小さい項があったとしてもこれらは同じオーダーである、という点が挙げられる。例えば、$O(n^3)$と$O(5n^3)$と$O(3n^3+2n^2+\log n)$は同じオーダーである。
\\ \\　
ここで紹介した時間計算量や空間計算量は決して理論だけのものではない。実装方法を決める際に、求められている仕ように対して時間計算量/空間計算量を見積もることで、適切な実装を選ぶ一助とすることができる。主要な処理のオーダーを知っておけば、全体の見通しを立てやすい(例えば、strlen関数は、引数の文字列長$l$に対し、各回の呼び出し〜返却まで$O(l)$かかることが知られている)。また、計算回数を大雑把でよいので見積もれば、処理にかかる時間等も把握しやすい\footnote{現在のPCなら、大体1000万回〜1億回の処理で1秒である(1回の処理にかかる時間やPC性能により変動)。もちろん、計算時間を小さい$n$について測定し、そこから大きい$n$についての時間を推定するという手法も有効である。}。

\minisec{ソートの安定性}
バブルソートでは、最初に2つあった3の順序関係が入れ替わっていない。このように、同じ大きさの値が配列にある時、その順序関係が入れ替わらないソートを\textbf{安定なソート}\index{あんていなそーと@安定なソート}(Stable sort)と呼び、逆に入れ替わるソートを\textbf{不安定なソート}\index{ふあんていなそーと@不安定なソート}(Unstable sort)と呼ぶ。単なる値の整列だけならば気にしなくてよいが、構造体や文字列をソートする場合には安定性が問題になってくる場合がある。ソートによっては、不安定であっても比較的簡単に安定に組みなおすことが可能なものもある。

\minisec{バブルソートのまとめ}
以上に説明したバブルソートについてまとめておこう。なお、バブルソートは速度も遅く、これといった優位性も無いので、実装練習以外に使われることは稀である。
\begin{itembox}[l]{バブルソート}
\minisec{手順}
\begin{enumerate}
\item 配列を順にスキャンし、途中、大小関係が逆転している部分があれば、その2要素を交換する。
\item 第$k$回目のスキャンにおいて第$n-k+1$要素までスキャンを終えたら\footnote{効率化を考えて第$n-k+1$要素までとしたが、第$n$要素まで見るとする方が一般的である。もちろん、第$n$要素まで見るほうが遅い。}、1に戻る。但し、ソートが完了しているか$k=n$であれば終了する。
\end{enumerate}
\minisec{性質/特徴}
\begin{multicols}{3}
\begin{description}
\item[安定性] 安定
\item[時間計算量] $O(n^2)$
\item[空間計算量] $O(1)$
\end{description}
\end{multicols}
\end{itembox}

以下に紹介するソートでは、簡単な例示や特徴的な性質の説明などを主眼とし、時間計算量などはその結果のみを示すに留める。

\section{基本的なソート}
まず、あまり高速ではないがオーバーヘッドが小さく、比較的簡単に組めるソートを紹介する。小規模データや、後で扱う高速なソートと組み合わせて用いられることが多い。

\subsection{挿入ソート}
\textbf{挿入ソート}\index{そうにゅうそーと@挿入ソート}(insertion sort)ないし\textbf{単純挿入法}\index{たんじゅんそうにゅうほう@単純挿入法}は配列を順に見ていき、途中で大小関係の逆転している場所を見つけたら、後側の要素をそこまでの適切な場所\footnote{この探索には、後で説明する二分探索を用いると良い。二分探索を用いた挿入ソートを特に二分挿入ソート(binary insertion sort)と呼ぶこともある。}に「挿入」する方法である。ほぼソートされた配列に強く、ソート済み配列への要素追加などの際に高速である。
\begin{itembox}[l]{挿入ソート}
\minisec{手順}
\begin{enumerate}
\item 配列を順にスキャンしていく。
\item 途中、大小関係が逆転している部分があれば、その後ろ側の要素を、既にスキャンを終えた部分のうち適切な位置に挿入する。この時、配列をひとつずつ順繰りにずらさなければならない点に注意。
\item 配列の終端まで来たら終了する。
\end{enumerate}
\minisec{性質/特徴}
\begin{multicols}{2}
\begin{description}
\item[安定性] 安定
\item[時間計算量] 平均$O(n^2)$/最良$O(n)$　\\　
\item[空間計算量] $O(1)$
\item[付記] ほぼソート済み配列に対し高速
\end{description}
\end{multicols}
\end{itembox}

この手順に従って、[4,1,3,2,3]をソートすると、[1,4,3,2,3] $\rightarrow$ [1,3,4,2,3] $\rightarrow$ [1,2,3,4,3] $\rightarrow$ [1,2,3,3,4]となり、バブルソートよりも手順が少ないことがわかるだろう。

\subsection{選択ソート}
\textbf{選択ソート}\index{せんたくそーと@選択ソート}(selection sort)は「王様探し」などとも呼ばれるソートで、配列中から最小値を探し、それを先頭に持っていく方法である。挿入ソートと異なるのは、最小値を先頭に挿入して残りの要素を順繰りにずらすのではなく、元々先頭にあった要素と交換する点である。交換回数が最大$n-1$回である分、バブルソートより高速である。一般に選択ソートといった場合は不安定であるが、安定に実装するのも難しくない。
\begin{itembox}[l]{選択ソート}
\minisec{手順}
\begin{enumerate}
\item 配列を順にスキャンするが、第$k$回目のスキャンでは第$k$要素からスキャンをはじめる。$k=n$ならば終了する。
\item 最後までスキャンすることで最小要素を見つける。
\item 最小要素と第$k$要素を交換し、最初のステップに戻る。
\end{enumerate}
\minisec{性質/特徴}
\begin{multicols}{3}
\begin{description}
\item[安定性] 不安定(安定も可)
\item[時間計算量] $O(n^2)$
\item[空間計算量] $O(1)$
\end{description}
\end{multicols}
\end{itembox}

この手順に従って、[4,1,3,2,3]をソートすると、[1,4,3,2,3] $\rightarrow$ [1,2,3,4,3] $\rightarrow$ [1,2,3,3,4]となる(但し、交換が行われないスキャンを省いた)。

\subsection{シェーカーソート}
\textbf{シェーカーソート}\index{しぇーかーそーと@シェーカーソート}(shaker sort)ないし\textbf{カクテルソート}\index{かくてるそーと@カクテルソート}(cocktail sort)は、バブルソートに近いソートで「第$k$回目のスキャンを終えると後側$k$項が整列されている」という性質を使って改良したものである。まず、配列を前から見ていき、一番後ろまでバブルソートの要領で交換する。その後、後ろまでたどり着いたら、今度は反対向き(後ろから前)に向かって配列を見ていき、同様の要領で交換を行う。第$k$回目の往復では、第$k$要素から第$n-k+1$要素の間を往復することになる。
\\ \\　
このソートも挿入ソート同様にほとんどソートされた配列に対して高速である。但し、実装の長さや平均速度の面から、挿入ソートのほうが一般的である。

\begin{itembox}[l]{シェーカーソート}
\minisec{手順}
\begin{enumerate}
\item このステップを行うのが第$k$回目であるとき、第$k$要素から配列をスキャンし、逆順箇所を交換する。
\item 第$n-k+1$要素にたどり着いたら、今度は配列を逆に見て、逆順箇所を交換する。
\item 第$k$要素まで戻ってきたら、ステップ1に戻る。これを、往復区間がなくなる\footnote{毎回2ずつ往復区間が減り、ソートは1要素以下で終わりであるため、$k=\left[\frac{n}{2}\right]$回目の往復が終わったら、ソート完了とできるという事がわかる。但し、$[]$はガウス記号。}か交換が起こらなくなるまで繰り返す。
\end{enumerate}
\minisec{性質/特徴}
\begin{multicols}{2}
\begin{description}
\item[安定性] 安定
\item[時間計算量] $O(n^2)$　\\　
\item[空間計算量] $O(1)$
\item[付記] ほぼソート済み配列に対し高速
\end{description}
\end{multicols}
\end{itembox}

この手順に従って、[4,1,3,2,5,3]をソートすると、[1,4,3,2,5,3] $\rightarrow$ [1,3,4,2,5,3] $\rightarrow\cdots\rightarrow$ [1,3,2,4,3,5] $\rightarrow$ [1,3,2,3,4,5] $\rightarrow$ [1,2,3,3,4,5]となる。

\subsection{ノームソート}
\textbf{ノームソート}\index{のーむそーと@ノームソート}(gnome sort)\footnote{gnomeは庭師を示し、庭師が鉢植えを並べ替える方法に由来する。}は単純に実装できる割に、最良だと$O(n)$の安定ソートである。基本的にはバブルソート同様、順番にスキャンして逆順の部分を交換する。但し、交換が起きた時、1つ前に戻ってもう一度比較する、という手順が入る。
\begin{itembox}[l]{ノームソート}
\minisec{手順}
\begin{enumerate}
\item 配列を順にスキャンする。
\item 順序が逆の場所を見つけたら交換を行い、続きのスキャンを1つ前の要素から行うようにする。最後まで辿り着けば終了。
\end{enumerate}
\minisec{性質/特徴}
\begin{multicols}{2}
\begin{description}
\item[安定性] 安定
\item[時間計算量] 平均$O(n^2)$/最良$O(n)$　\\　
\item[空間計算量] $O(1)$
\item[付記] 一重ループで実装可能
\end{description}
\end{multicols}
\end{itembox}

この手順に従って、[4,1,3,2,3]をソートすると、[1,4,3,2,3] $\rightarrow$ [1,3,4,2,3] $\rightarrow$ [1,3,2,4,3] $\rightarrow$ [1,2,3,4,3] $\rightarrow$ [1,2,3,3,4]となる。

\subsection{シェルソート}
\textbf{シェルソート}\index{しぇるそーと@シェルソート}(Shell Sort)\footnote{シェル(Shell)は開発者の名前 Donald L. Shell からとっており、貝殻とは関係ない。}は挿入ソートを改良したものである。「大雑把に見ていってだんだん細かくする」という考えと「ソート済み配列に対して挿入ソートは高速である」という考えが元となっている。まず、大雑把に間隔を開けて挿入ソートを行う。その後、この間隔を少しずつ小さくしていくという手法である。
\begin{itembox}[l]{シェルソート}
\minisec{手順}
\begin{enumerate}
\item 適当な間隔$h$を定める。通常、これは2の累乗数にすることが多い。
\item 間隔$h$をあけてとった数列に対して、挿入ソートを適用する。なお、このような間隔$h$の数列は配列中に複数存在する為、挿入ソートも複数回行われる。
\item 間隔$h$を適度に狭めて(2の累乗数にしている場合は半分にすることが多い)先と同様の操作を行う。
\item $h=1$になるまでこれを繰り返してソートすることができる。
\end{enumerate}
\minisec{性質/特徴}
\begin{multicols}{3}
\begin{description}
\item[安定性] 不安定
\item[時間計算量] $O(n \log^2 n)$
\item[空間計算量] $O(1)$
\end{description}
\end{multicols}
\end{itembox}

なお、このソートで間隔$h$を$h_{n+1}=3h_n+1$という漸化式から生み出される数列$\{1,4,13,40,\cdots\}$にすると、時間計算量が$O(n^{1.25})$になることが知られている。

シェルソートはここまでに紹介したソートに比べ、やや速く、中規模データの並び替えに便利である(ここまでのソートは小規模向け、この後のものは大規模向けが基本である)。中規模向けであることから、後述の「高速なソート」に、挿入ソートなどと共に組み合わせられることが多い。
\\ \\　
数列[8,3,1,2,6,5,6,4]を間隔4,2,1の順に変化させてシェルソートしてみよう。まず、間隔4なので、[8,3,1,2,6,5,6,4] $\rightarrow$ [6,3,1,2,8,5,6,4] となる。ここから、間隔2と間隔1を順に適用して[1,2,6,3,6,4,8,5] $\rightarrow$ [1,2,3,4,5,6,6,8]となる。

\section{高速なソート}
比較ソート(個々の項目を比較演算で大小判定することを基本とするソート)の限界速度は$O(n\log n)$であることが理論的に知られている。比較ソートは汎用性が高くメモリもそれほど多く食うわけではないため、$O(n\log n)$のソートは実用的である。これよりも速いソートはデータの特殊性や特別なハードウェアなどが必要な場合があるが、もしも適用できるならば有効である。ここでは、$O(n\log n)$の比較ソートを中心に、それ以外のソートとしてバケットソートを紹介する。
\subsection{マージとマージソート}
マージソートはD\&Cの一種で、配列の各部分をソートし、それらに対してマージと呼ばれる操作を行うことによって全体をソートしようとするものである。
\minisec{マージ}
\textbf{マージ}\index{まーじ@マージ}(merge)とは、2つのソート済み配列を統合し、1つのソート済み配列にする操作である。2つの配列をキューに見立て、先頭要素同士を比較して、小さい側を統合後の配列にいれればよいので、次のように非常に簡単に実装できる。
\begin{boxnote}
\minisec{マージの実装}
ポインタ\verb|p1,p2|によって示される2つのint型配列をマージし\footnote{ここではint型昇順という縛りをつけて組んだが、より一般化することもできる。実際、stdlib.hに収録されているソートの関数(後述)は、汎用ポインタ・要素あたりのサイズ・個数などを用いて一般化されている。規則についての一般化は簡単には思いつかないかもしれない。だが、順序付けを関数によって行うものとし(一般に\textbf{比較関数}\index{ひかくかんすう@比較関数}(comparison function)と呼ばれる)、その関数のポインタを引数に取れば、汎用化できる(前述のソートの関数も引数に比較関数をとっている)。}、\verb|p3|に示される配列に格納する関数を作成する。\verb|p1,p2|の配列は各々\verb|n1,n2|個の要素を持つものとし、\verb|p3|の要素数は\verb|n1+n2|以上あるものとする。
\end{boxnote}

\begin{boxnote}
\begin{lstlisting}[caption=マージ,label=program14_1]
void merge(int *p1,int *p2,int *p3,int n1,int n2){
  int i,p1_suf=0,p2_suf=0,tmp=n1+n2;
  for(i=0;i<tmp;i++){
    if(p1_suf>=n1) *(p3+i)=*(p2+p2_suf++);
    else if(p2_suf>=n2) *(p3+i)=*(p1+p1_suf++);
    else 
    *(p3+i)=(*(p1+p1_suf)<*(p2+p2_suf))?*(p1+p1_suf++):*(p2+p2_suf++);
  }
}  
\end{lstlisting}
\end{boxnote}

要素数$n_1,n_2$の2つの配列をマージする場合、それにかかる時間は$O(n_1+n_2)$である。

\minisec{マージソート}
2つのソート済み配列をマージすれば、それによってできる統合後の配列はソート済みである。従って、ある配列を前半と後半の2つに分けて、その各々をソートした後にマージする、という戦略を考えることができる。つまり、配列を分割し、統治(ソート)しているのである。これを再帰的に適用していくのが\textbf{マージソート}\index{まーじそーと@マージソート}(merge sort)である。
\\ \\　
ここで、何故分割統治するのかという事を考えてみよう。1段だけの分割を考える。$2n$要素の配列を挿入ソートで整列する場合、挿入ソートは$O(n^2)$であるため、その比較回数は定数$C$を用いて$4Cn^2$である\footnote{ここでは均等分割しているが、奇数の場合は前半ないし後半の要素数を1だけ大きくすれば良い。できるだけ均等にする理由としては$n_1+n_2=N\ (n_1,n_2,N\in\mathbb{N})$に対して$n_1^2+n_2^2$を最小化する問題を考えてみれば良い。}。だが、前半と後半に分け、各々$n$要素を挿入ソートするとすれば、ソートにかかる比較回数は$2Cn^2$ですむ。マージは$O(n)$であるので、$n$が十分大きいとき、分割してソートした後にマージする方法は、単純にソートする方法のおよそ半分の計算回数になり\footnote{$n$が小さい場合、「ソートする配列が半分になって比較の減った分」に対する「マージに必要になった比較の増えた分」の割合が大きくなるので、高速化が望めるかどうか怪しくなる。このような場合の対策については後述する。同じ事で、3分割以上の分割をした場合も、マージに必要な比較が増えてしまうため、小さい$n$に対して高速化が望めるかどうかは怪しい。}、高速化できるのである。これは、マージソートのみにとどまらず、D\&Cを用いるソート全体に共通していることである。
\\ \\　
このように、分割は計算回数を減らすことができ、しかも再帰的に適用可能である。マージソートの場合、各部分列に対して再帰的にマージソートを適用し、要素数1の配列ができたらそれを「ソート済み配列」として戻ってマージすれば良い。この時、部分列の数は再帰1段に付き2倍になり、これらをマージして戻すのに$n$回程度の比較がある。このことから考えてわかるとおり、$O(n)$のマージを$\log_2 n$回程度行う必要があるので、マージソートは$O(n\log n)$で動作する。この過程において配列要素の値が関係していないことからわかるとおり、マージソートはどのような配列に対しても$O(n\log n)$で動作してくれる。但し、マージのためのサブ配列が$O(n)$程度必要である。以上をまとめると次のようになる。
\begin{itembox}[l]{マージソート}
\minisec{手順}
\begin{enumerate}
\item 配列の要素数が1であればそれをソート済みとして終了する。
\item 配列を前半と後半に(ほぼ)均等に分割する。
\item 配列の前半・後半に各々マージソートを再帰的に適用する。
\item 配列の前半・後半をマージし、ソート済み配列を作って終了する。
\end{enumerate}
\minisec{性質/特徴}
\begin{multicols}{3}
\begin{description}
\item[安定性] 安定
\item[時間計算量] $O(n \log n)$
\item[空間計算量] $O(n)$\footnote{$O(1)$で動作するIn-placeマージソートというアルゴリズムもある。}
\end{description}
\end{multicols}
\end{itembox}

\minisec{少要素数への対応}
先の脚注でも説明したが、マージソートを始めとするD\&C型ソートは、少要素数の場合に遅くなりがちである(ないし、高速化される見込みがなくなる)。仮に4要素の配列に対してマージソートを適用してみると、$n\log n$=8となってしまい、定数によってはマージソートのほうが$O(n^2)$のソートよりも遅いことも十分あり得る。しかし、分割統治でアルゴリズムを組んでいる限り、このような少要素数のソートが部分問題として出てきてしまう。そこで、これに対する対応を考えてみる。
\\ \\　
マージソートを再帰的に適用するのは、要するに部分列をソートするためであった。そこで、部分列が十分小さい(高速化が見込めない)時には、それを別のアルゴリズムによってソートすれば良い。例えば、要素数が8以下ならば（ソート済みorほぼソート済みの可能性が高いため）挿入ソートを適用するなどの方法がある(不安定でも良いなら、もう少し多い段階でシェルソートを用いても良い)。

従って、先にまとめたマージソートの第1ステップを「要素数が一定数以下であれば挿入ソートを適用して終了する」に変更すれば、少要素数の場合の問題が解消され、高速化が見込めるだろう。なお、この「小規模の時挿入ソート、中規模の時シェルソートで部分問題を解く」というテクニックは、マージソートにとどまらないテクニックである\footnote{同じ事で、D\&Cアルゴリズムに対しては、それを再帰適用して部分問題が十分小さくした後、部分問題を別の方法で解いて統合していくことが多い。この理由も本節で述べたのと同じものである。このことから、マージソートはD\&Cアルゴリズムの縮図と言ってよいだろう。}ので、他のD\&Cソートの実装の際にも使ってみて欲しい。

\subsection{クイックソート}
\textbf{クイックソート}\index{くいっくそーと@クイックソート}もマージソート同様D\&Cの一種であり、最悪は$O(n^2)$だが、平均的には$O(n\log n)$の高速なソートで、比較ソートの中ではトップレベルの平均速度である。

\minisec{クイックソートのアルゴリズムと乱択}
クイックソートは、まず適当な値(\textbf{ピボット}\index{ぴぼっとく@ピボット(クイックソート)}(pivot))を定め、これより小さい値を前側に、大きい値を後側に集める。その後、ピボットの部分を切れ目にして分割し、これを再帰的に適用する（分割区間は半分ずつとは限らない）。ピボットは通常、配列の中の要素から定められる。

クイックソートにおいて重要なのはピボットの選択である。仮に、各部分列において最大値/最小値ばかりをピボットに選んでしまった場合、これは最悪のケースとなり$O(n^2)$の計算回数が必要になる。そこで、乱数を用い、選ぶピボットの規則をなくしてやることにより、最悪のケースを回避しやすくする方法が知られている(乱択クイックソート)\footnote{この方法も決して万能ではない。1が10000個、10000が10000個、2〜9999の値が500個というような配列のソートでは思ったより速度がでないこともありうる。理想的には、各部分列で中央値を求めてピボットに用いれば良いのであるが、中央値を求めるには$O(n)$かかるため、高速化のために中央値を用いたはずがそれほどの効果が見込めない(あるいは逆に遅くなる)こともありうるのが難点である。}。他にも最悪のケースを回避する方法は幾つもあるが、実装の際には、ひとまず乱択クイックソートを作ることができればよいだろう。以下、乱択クイックソートについて記す。
\begin{itembox}[l]{(乱択)クイックソート}
\minisec{手順}
\begin{enumerate}
\item 要素数が1であるならば確定したものとして終了する。
\item 乱数によってピボットを1つ選ぶ。
\item 左から順に見て、ピボット以上のものを見つけたらその位置を$i$とする。同様に、右からも見て、ピボット以下のものを見つけたらその位置を$j$とする。
\item $i$が$j$より左ならば、その2つの要素を入れ替えて、探索に戻る($i$は右に、$j$は左に一つ進める)。そうでない場合は次に進む。
\item $i$の左側を境界に分割を行って2つの領域に分け、そのそれぞれに対して本アルゴリズムを再帰的に適用する。
\end{enumerate}
\minisec{性質/特徴}
\begin{multicols}{2}
\begin{description}
\item[安定性] 不安定
\item[時間計算量] 平均$O(n \log n)$/最悪$O(n^2)$
\item[空間計算量] $O(n)$($O(\log n)$にもできる)
\end{description}
\end{multicols}
\end{itembox}

なお、ピボットを選ぶ方法としては、乱択で3つとってその中央値を取るなどの方法も考えられる。そのため、先の手順の2は必要に応じて変更すれば良い。

クイックソートもD\&Cであるので、マージソート同様、少要素部分列に対しては挿入ソートを適用するように組み替えて高速化をはかることができる。

\minisec{qsort関数と比較関数}
C言語にはクイックソートに由来した\footnote{名前の由来がクイックソートであるだけで、実際に中のアルゴリズムがクイックソートである必要はない。極端な話、バブルソートやストゥージソート、ボゴソートなどの実用に向かないレベルのソートであっても仕様は満たしていることになる。とはいえ、主要なコンパイラは十分に速いソートで実装しているので心配ない。}、ソートを行う関数\verb|qsort|がstdlib.hに用意されている。これを使ったソートの例を見ておこう。
\begin{boxnote}
\minisec{qsort関数によるソート}
qsort関数を用いてint型配列を昇順整列する。
\begin{lstlisting}[caption=qsort関数の例,label=program14_2]
#include<stdio.h>
#include<stdlib.h>

#define SIZE_AR(x) ((sizeof(x))/(sizeof(x[0])))

int compare(const void *p1, const void *p2);

int main(void){
  int data[5]={1,9,2,0,-4},i;
  qsort(data,SIZE_AR(data),sizeof(int),compare);
  for(i=0;i<SIZE_AR(data);i++)
    printf("%d\n", data[i]);
  return 0;
}

int compare(const void *p1, const void *p2) {
  int n1,n2;
  n1=*((const int *)p1);
  n2=*((const int *)p2);
  return n1-n2;
}
\end{lstlisting}
\end{boxnote}

リスト\ref{program14_2}の$l$.10において、qsort関数を用いている。この関数は第1引数にソートしたい配列へのポインタを、第2引数に整列したい要素の数を、第3引数に1要素あたりの要素のサイズをとってソートする。第4引数の\textbf{比較関数}\index{ひかくかんすう@比較関数}(comparison function)への関数ポインタは、ソート順序の規則を表す。

qsort関数(及びbsearch関数)の比較関数には、次のような条件が要請される。
\begin{itembox}[l]{比較関数の条件}
\begin{itemize}
\item 引数が2つで、共に\verb|const void *|型であること。
\item 返却値がint型であること。
\item 第1引数と第2引数を比較し、第1引数が第2引数より前に来るべき時は負値を、第1引数が第2引数より後にくるべき時は正値を、2つを等しいとみなす場合には0を返すようにする\footnote{strcmp関数が同じような形の返却値を取るので、参考にすると良い。但し、strcmp関数そのものは引数の型の要件を満たさない。}。
\end{itemize}
\end{itembox}
上記の条件を満たす比較関数を適切に定義することで、qsort関数を用いて任意のデータを整列できる(構造体を含む)。

\subsection{ヒープソート}
親ノードが子ノードより小さいという関係性で構築されたヒープにおいては、ルートノードは最小値を示している。そして、ヒープの再構築にかかる時間計算量は$O(\log n)$である。これを利用して、ヒープを用いて選択ソートを行う\footnote{二分探索木や平衡二分探索木を用いる方法もある。}ソートのことを\textbf{ヒープソート}\index{ひーぷそーと@ヒープソート}(heap sort)と呼ぶ。不安定であり、平均的にはクイックソートよりも遅いが、空間計算量は$O(1)$であり、最悪時間計算量$O(n\log n)$なので、使い勝手のいいソートだと言える。
\begin{itembox}[l]{ヒープソート}
\minisec{手順}
\begin{enumerate}
\item 配列内の全要素を用いてヒープを構築する。
\item 構築されたヒープのルートノードの値を配列に格納する。
\item ルートノードを削除し、要素が残っていなければ終了する。残っている場合、ヒープを再構築し、前項に戻る。
\end{enumerate}
\minisec{性質/特徴}
\begin{multicols}{3}
\begin{description}
\item[安定性] 不安定
\item[時間計算量] $O(n \log n)$
\item[空間計算量] $O(1)$
\end{description}
\end{multicols}
\end{itembox}

残りの要素が少ない場合、挿入ソートを使って高速化することもある。

\subsection{コムソート}
\textbf{コムソート}\index{こむそーと@コムソート}(comb sort)は挿入ソートをシェルソートに変えたのと同様の方法で、バブルソートを「大雑把に見てから細かく」適用したものである。理論上$O(n^2)$だが、実際には$O(n\log n)$で動く。速度の割に実装が簡単でコードも短いので、比較的使いやすいソートである。但し、不安定である点には注意されたい。

シェルソート同様に、ある一定の間隔を空け、その間隔毎にバブルソートを行う。第$k$回目の間隔$h_k$は漸化式
\begin{equation}
h_k=\left[\frac{h_{k-1}}{1.3}\right]\ ,\ h_0=n \label{comb}
\end{equation}
によって計算される(但し、$[ ]$はGauss記号)。以上、まとめて次のようになる。
\begin{itembox}[l]{コムソート}
\minisec{手順}
\begin{enumerate}
\item $k=1$とし、式(\ref{comb})により$h_k$を計算する。
\item 配列の第$i$項と第$i+h_k$項(但し、$i=0,1,\cdots,n-h_k$)を比較し、前者のほうが大きい場合は入れ替える。
\item $h_k\neq 1$であるならば$k$を1増やして、$h_k$を計算し、前項に戻る。$h_k=1$ならば、正しくソートされるまでバブルソートを繰り返して終了する。
\end{enumerate}
\minisec{性質/特徴}
\begin{multicols}{3}
\begin{description}
\item[安定性] 不安定
\item[時間計算量] $O(n \log n)$
\item[空間計算量] $O(1)$
\end{description}
\end{multicols}
\end{itembox}

\subsection{ストランドソート}
\textbf{ストランドソート}\index{すとらんどそーと@ストランドソート}(strand sort)はマージを利用するソートであるが、マージソートとは違いD\&Cには属さない。性能はマージソートに劣るが、実装は楽である。

このソートは昇順部分配列の抽出とマージを繰り返すだけである。手順を示そう。
\begin{itembox}[l]{ストランドソート}
\minisec{手順}
\begin{enumerate}
\item 元の配列の他に、サブ配列と結果配列を用意しておき、次の2つの操作を元の配列の要素がなくなるまで繰り返す。
\item 配列を前から見ていき、昇順関係が崩れないようにサブ配列に要素を移す。(抽出)
\item サブ配列を結果配列にマージし、前項に戻る。(マージ)
\end{enumerate}
\minisec{性質/特徴}
\begin{multicols}{2}
\begin{description}
\item[安定性] 安定
\item[時間計算量] 平均$O(n \log n)$/最悪$O(n^2)$
\item[空間計算量] $O(n)$
\end{description}
\end{multicols}
\end{itembox}

手順を聞くだけではわかりづらいので、配列[3,7,1,9,5,7,4,6,8,2]をストランドソートした場合の変化を表\ref{strand}にまとめた。
\begin{table}[htb]
\centering
\caption{ストランドソートの変化(操作の後の数字は、その実行が何回目かを示す)}\label{strand}
\begin{tabular}{|c||c|c|c|}\hline
操作&元の配列&サブ配列&結果配列\\ \hline
&&& \\[-15pt] \hline
初期状態&[3,7,1,9,5,7,4,6,8,2]&[]&[] \\ \hline
抽出1&[1,5,7,4,6,8,2]&[3,7,9]&[] \\ \hline
マージ1&[1,5,7,4,6,8,2]&[]&[3,7,9] \\ \hline
抽出2&[4,6,2]&[1,5,7,8]&[3,7,9] \\ \hline
マージ2&[4,6,2]&[]&[1,3,5,7,7,8,9] \\ \hline
抽出3&[2]&[4,6]&[1,3,5,7,7,8,9] \\ \hline
マージ3&[2]&[]&[1,3,4,5,6,7,7,8,9] \\ \hline
抽出4&[]&[2]&[1,3,4,5,6,7,7,8,9] \\ \hline
マージ4&[]&[]&[1,2,3,4,5,6,7,7,8,9] \\ \hline
\end{tabular}
\end{table}

ストランドソートはコムソートと並んで実装が楽なので、そこそこ高速な安定ソートが欲しい場合などにさっと実装できるのが利点である。

\subsection{イントロソート}
gccのC++のソートに使われているソートアルゴリズムが、ここで紹介する\textbf{イントロソート}\index{いんとろそーと@イントロソート}(intro sort)である。これは、クイックソートの弱点をヒープソートでカバーしたソートアルゴリズムである。

クイックソートの弱点として、よく工夫された並びに対しては、実行時間が$O(n^2)$になってしまうという点が挙げられる。実際、これを突いたDos攻撃も存在する。そこで、通常はクイックソートを使うことにしておき、再帰回数が増えた場合、その各部分列に対するソートをヒープソートに切り替える。更に、ヒープソートの最終段階において、挿入ソートを用いる。このように、クイックソート・ヒープソート・挿入ソートを組み合わせることで高速なソートを組むことができる。これがイントロソートである。
\\ \\　
イントロソートは平均/最悪時間計算量$O(n\log n)$で、空間計算量$O(\log n)$の、不安定ソートである。

\subsection{ティムソート}
PythonやJava7、あるいはC++のStable\_sortなどで使われる高速ソートアルゴリズムが\textbf{ティムソート}\index{てぃむそーと@ティムソート}(Tim sort)である。マージソートを基本にしているが、Top-Downではなく、Bottom-Upに使う部分が違う。

配列を先頭から順に見ていき、最初の2項を含む広義単調増加部分ないし狭義単調減少部分を見つける。狭義単調減少部分の場合、これを逆転させて狭義単調増加部分にする。この単調増加部分を為す要素の数が一定個数$S$未満の場合、それ以降の部分に対して二分挿入ソートを行い、各分割が一定の大きさ$S$になるようにする。この時、「一定の大きさ」$S$は、配列の個数$n$に対し、$2^kS=n$($k$は自然数)という関係を満たし、かつ挿入ソートが最速である程度に小さい(概ね32個未満)ことが望ましい。条件を満たす$S$が複数ある場合、最大のものを選ぶ。

これによって配列が大きさ$S$の分割になったら、隣り合ったメモリ領域をマージする。この時、全要素をマージするのではなく、前側分割において後側分割の先頭より小さいものと、後側分割において前側分割の終端より大きいものを除いた部分列をマージすることで、マージ速度をあげる。実際には、この部分列の作成も二分探索等を用いて高速化する。
\\ \\　
以上、マージソートに対して各種の高速化を用いたものがティムソートである。もともと並んでいるデータに対して$O(n)$、平均/最悪の場合$O(n\log n)$で動作する安定ソートで、いいことづくめのようであるが、残念ながら空間計算量$O(n)$を要する他、実際の実装もやや難しい。紹介程度に捉えておくと良い\footnote{安定ソートが必要な場合、実用的には部分列が十分小さくなった時に二分挿入ソートを使うようにしたマージソートで十分速く、実装も楽である。主に「既に組まれているものを使う」ぐらいであろう。}。

\subsection{バケットソート}
\textbf{バケットソート}\index{ばけっとそーと@バケットソート}(Bucket sort)は非常に別名が多いソートで、\textbf{分布数えソート}\index{ぶんぷかぞえそーと@分布数えソート|see{バケットソート}}(Distribution counting sort)や\textbf{ビンソート}\index{びんそーと@ビンソート|see{バケットソート}}(bin sort)とも呼ばれる。これは比較ソートではなく、また別の手段による方法である。
\\ \\　
試験の得点で順位を決める場合、分布を求めて順位を決定するだろう。この、分布を数えて順位付けを行い、その順位付けに応じてソートを行う方法が、バケットソートである。

バケットソートを行う場合は、要素が有限種類に限られなければならない。要素が$m$種類であるならば、空間計算量は$O(m)$となる。但し、この条件がクリアできた場合、計算時間$O(n)$で安定という大きな恩恵を得ることができるソートでもある。
\begin{itembox}[l]{バケットソート}
\minisec{手順}
\begin{enumerate}
\item 要素の取りうる値$m$に対し、$m$個のキューを用意しておく。この時、キューは要素順に対応するようにしておく。
\item 配列を順に見て、各要素を対応したキューに挿入する。
\item キューから要素を取り出していく。取り出し終えたら次のキューに移る。
\end{enumerate}
\minisec{性質/特徴}
\begin{multicols}{2}
\begin{description}
\item[時間計算量] $O(n)$
\item[安定性] 安定(キューを用いた場合) 　\\　
\item[空間計算量] $O(m)$
\item[条件] 要素の定義域が有限個の定値
\end{description}
\end{multicols}
\end{itembox}

この手順において「キューは要素順に対応するように」というのは、例えばテストの例の場合、100点のキューを\verb|queue[100]|,99点のキューを\verb|queue[99]|,$\cdots$などとしておく、ということである。この条件を満たすように定めれば、第3ステップが「100点のキューの要素を全て取り出して配列に格納する」「99点のキューの要素を全て取り出して配列に追加する」$\cdots$となり、たしかに$O(n)$になるのである。
\\ \\　
ここに述べたとおり、バケットソートは性能が良いソートであるが、メモリとのトレードオフになる。そのため、例えば電話番号の整列など、存在するかしないかだけの場合にはビットを立てるなどの方法でキューを表現してメモリを節約したほうが良い。

また、この方法を用いて大雑把に分割してソートを行う方法もある(キューの代わりにプライオリティーキューを用いる)。ここで紹介した基本形を元に、様々な工夫を加えて使えるソートである。

\section{サーチとその手法}
\textbf{サーチ}\index{さーち@サーチ}(search)は、必要な値を配列内から検索するために用いられる。サーチもソート同様、非常に多くのアルゴリズムが知られているが、ここでは紙面の都合上、リニアサーチとバイナリサーチを紹介するに留める。以下、$n$項のint型配列に目標の整数が含まれているかどうかを判定し、含まれているときにその位置を見つける方法\footnote{このように、あるかないかの判別までするのがサーチの問題の一般的な形である。}について記す。
\subsection{リニアサーチ}
\textbf{リニアサーチ}\index{りにあさーち@リニアサーチ}(linear search)は誰しもがわかる方法で、配列を前から順に見ていく方法である。日本語では\textbf{線形探索}\index{せんけいたんさく@線形探索}などと呼ばれる。時間計算量$O(n)$で動作する。

この方法は他のサーチの方法と違って前提条件がないため、少ない回数の探索などの場合には便利である。しかし、同じ配列を何度も探索する場合、前提条件を満たすようにしてから他のサーチアルゴリズムを使ったほうが効率が良いだろう。

\subsection{バイナリサーチ}
\textbf{バイナリサーチ}\index{ばいなりさーち@バイナリサーチ}(binary search)は日本語では\textbf{二分探索}\index{にぶんたんさく@二分探索}とも呼ばれるサーチで、配列がソートされていることを前提とし、$O(\log n)$で要素を見付け出す(あるいは無い事を示す)アルゴリズムである。

\minisec{bsearch関数}
バイナリサーチそのものの説明の前に、関数を紹介しておこう。stdlib.hには、バイナリサーチに由来した\footnote{qsort同様、バイナリサーチのアルゴリズムで組まれているかどうかは保証されていない。}、bsearch関数がある。この関数は、第1引数に、探したい値へのポインタを取る。残りの引数はqsortと同様である(但し、第2引数の配列は、第5引数の比較関数の規則において昇順ソート済みである必要がある)。これにより、bsearch関数は、一致する要素があればそれへのポインタを、存在しなければNULLポインタを返す。注意すべき点として、複数の同じ要素がある場合、そのどれが返されるかは不明である(が、乱択では無いので一定ではある)という点である。
\\ \\　
ひとまず多数の回の探索を行う！という場合には、配列をqsortした後bsearchを使うのが定番である。(リニアサーチと時間計算量を比較してみよ。)

\minisec{バイナリサーチのアルゴリズム}
それでは、バイナリサーチについて見ていくことにしよう。
\\ \\　
ソートされているならば、配列の真ん中の値を取ることで中央値がわかる。探している値がこの中央値よりも小さければ、その値は前半にしかない。逆に、この中央値よりも大きければその値は後半にある。そこで、その半分の部分列について、中央値と探している値を比較してやる。これを繰り返していくと、探索範囲が順次半分になるため、$O(\log n)$程度で配列の中に要素があるかどうかを判定できる。これがバイナリサーチのアルゴリズムの考え方である。
\begin{itembox}[l]{バイナリサーチのアルゴリズム}
\begin{enumerate}
\item 「今から注目する部分列」を配列全体にしておく。
\item 「今から注目する部分列」の中央値と探したい値を比較する。一致していれば終了し、一致していなければ次項へ進む。
\item 探したい値が中央値より小さいならば「今から注目する部分列」を前半分に縮小する。逆に、大きい場合は後半分に縮小する。要素数が0になった場合は見つからなかったとして終了し、そうでない場合は前項に戻る。
\end{enumerate}
\end{itembox}

バイナリサーチのアルゴリズムで重要なのは、これは配列の探索にかかわらず用いられる、という事である。例えば大小関係によって整数を当てるゲームでは、その整数の範囲に対して二分探索を行うことで解を求めることができる。

\minisec{範囲なしのバイナリサーチ}
バイナリサーチを座標などに用いる場合(例えば、どこまで池かを知りたい時など)、無限に広い座標では「中央値」を定めることができない。そこで、ひとまず元となる1点を決めておき、そこから1移動した点、1+2移動した点、1+2+4移動した点、$\cdots$と探索を行なっていく。そして、その結果が異なるようになった2点の間でバイナリサーチを行う。先の池の例であれば、点0が池で、1,3,7,15の距離にある点も池であるが、31の距離にある点が池でなかった場合、距離15から距離31の間の16の距離をバイナリサーチすることで池の端を見つけることができる。

この他、範囲の有無にかかわらずバイナリサーチを用いるアルゴリズムは多い。例えば、次講で学ぶ、方程式の解を求める二分法はまさしくバイナリサーチである。このように、バイナリサーチの考えは各所で出てくるため、ぜひ理解しておいていただきたい。

\newpage

\begin{shadebox}
\section*{本講の要点}
本講義では、ソートを中心に学び、それと関連の深いサーチについても触れた。
\subsection*{ソート}
\begin{itemize}
\item ソートは与えられたデータを一定の規則に従って整列する問題である。
\item ソートの規則は比較関数によって与えられる場合が多い。qsort関数に対する比較関数については、strcmpと似たような条件が定まっている。
\item ソートの性能を評価するための指標としては、時間計算量・空間計算量の他、同一値の複数要素の順序が変わるかどうか(安定性)などがある。
\item 今回紹介したソートは次のとおりである。
\begin{center}
\begin{tabular}{|c||c|c|c|}\hline 
名称&計算時間量(平均/最悪)&空間計算量&安定性\\ \hline
&&& \\[-15pt] \hline
バブル&$O(n^2)$&$O(1)$&安定\\ \hline
挿入&$O(n^2)$&$O(1)$&安定\\ \hline
選択&$O(n^2)$&$O(1)$&不安定\\ \hline
シェーカー&$O(n^2)$&$O(1)$&安定\\ \hline
ノーム&$O(n^2)$&$O(1)$&安定\\ \hline
シェル&$O(n\log^2 n)$&$O(1)$&不安定\\ \hline
マージ&$O(n\log n)$&$O(n)$&安定\\ \hline
クイック&$O(n\log n)$/$O(n^2)$&$O(\log n)$&不安定\\ \hline
ヒープ&$O(n\log n)$&$O(1)$&不安定\\ \hline
コム&$O(n\log n)$&$O(1)$&不安定\\ \hline
ストランド&$O(n\log n)$/$O(n^2)$&$O(n)$&安定\\ \hline
イントロ&$O(n\log n)$&$O(\log n)$&不安定\\ \hline
ティム&$O(n\log n)$&$O(n)$&安定\\ \hline
バケット&$O(n)$&$O(m)$&安定\\ \hline
\end{tabular}
\end{center}
\end{itemize}

\subsection*{サーチ}
\begin{itemize}
\item サーチとは、与えられたデータの中に探したいデータが存在するか判定し、存在するならどこに存在するか求める問題である。
\item リニアサーチは、前から順に要素を探索していく方法である。
\item バイナリサーチは、ソート済みの配列に対してその中央値と探索値を比較し、範囲を狭めていく方法である。
\item バイナリサーチの考え方は様々なアルゴリズムに応用される。
\end{itemize}
\end{shadebox}
